# Author: Peng Fei

from typing import List, Dict, Any, Optional, Callable
from src.models import (
    CandidateEvaluation,
    ScoringDimensions,
    JobRequirement,
    EvaluationStatus
)
from src.prompts import (
    REPORT_HEADER_TEMPLATE,
    BASIC_INFO_TABLE_TEMPLATE,
    DIMENSION_TABLE_TEMPLATE,
    OVERALL_RANKING_TEMPLATE,
    RECOMMENDATION_SUMMARY_TEMPLATE,
    RECOMMENDATION_STATUS,
    SCORE_THRESHOLDS
)
import re
from datetime import datetime

class ReportGenerationNode:
    """æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹ - å°†è¯„åˆ†ç»“æœè½¬æ¢ä¸ºMarkdownæ ¼å¼è¡¨æ ¼"""
    
    def __init__(self):
        pass
    
    def process(self, 
                evaluations: List[CandidateEvaluation],
                job_requirement: JobRequirement,
                scoring_dimensions: ScoringDimensions) -> Dict[str, Any]:
        """å¤„ç†æŠ¥å‘Šç”Ÿæˆ"""
        try:
            if not evaluations:
                return {
                    "status": "error",
                    "error": "æ²¡æœ‰å€™é€‰äººè¯„ä»·æ•°æ®",
                    "report": ""
                }
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_markdown_report(evaluations, job_requirement, scoring_dimensions)
            
            return {
                "status": "success",
                "report": report,
                "candidate_count": len(evaluations),
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "report": ""
            }

    async def process_stream(self, 
                            evaluations: List[CandidateEvaluation],
                            job_requirement: JobRequirement,
                            scoring_dimensions: ScoringDimensions,
                            progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """å¤„ç†æŠ¥å‘Šç”Ÿæˆï¼ˆå¸¦è¿›åº¦æµå¼è¾“å‡ºï¼‰"""
        try:
            if progress_callback:
                await progress_callback({
                    "stage": "report_generation",
                    "message": "å¼€å§‹ç”Ÿæˆå€™é€‰äººè¯„ä¼°æŠ¥å‘Š",
                    "progress": 90,
                    "total_items": 1,
                    "completed_items": 0
                })
            
            if not evaluations:
                return {
                    "status": "error",
                    "error": "æ²¡æœ‰å€™é€‰äººè¯„ä»·æ•°æ®",
                    "report": ""
                }
            
            if progress_callback:
                await progress_callback({
                    "stage": "report_generation",
                    "message": "åˆ†æå€™é€‰äººæ•°æ®",
                    "progress": 92,
                    "current_item": "æ•°æ®åˆ†æ"
                })
            
            # ç”ŸæˆæŠ¥å‘Šï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œä»¥é¿å…é˜»å¡ï¼‰
            import asyncio
            loop = asyncio.get_event_loop()
            
            if progress_callback:
                await progress_callback({
                    "stage": "report_generation",
                    "message": "ç”ŸæˆMarkdownæŠ¥å‘Š",
                    "progress": 95,
                    "current_item": "æŠ¥å‘Šç”Ÿæˆ"
                })
            
            report = await loop.run_in_executor(
                None, 
                self._generate_markdown_report, 
                evaluations, 
                job_requirement, 
                scoring_dimensions
            )
            
            if progress_callback:
                await progress_callback({
                    "stage": "report_generation",
                    "message": "æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
                    "progress": 98,
                    "current_item": "æŠ¥å‘Šå®Œæˆ"
                })
            
            return {
                "status": "success",
                "report": report,
                "candidate_count": len(evaluations),
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            if progress_callback:
                await progress_callback({
                    "stage": "report_generation",
                    "message": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}",
                    "progress": 90,
                    "error": str(e)
                })
            
            return {
                "status": "error",
                "error": str(e),
                "report": ""
            }
    
    def _generate_markdown_report(self, 
                                evaluations: List[CandidateEvaluation],
                                job_requirement: JobRequirement,
                                scoring_dimensions: ScoringDimensions) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report_parts = []
        
        # 1. æŠ¥å‘Šå¤´éƒ¨ä¿¡æ¯
        report_parts.append(self._generate_header(job_requirement, len(evaluations)))
        
        # 2. ç®€åŒ–çš„å€™é€‰äººè¯„ä¼°æ±‡æ€»è¡¨æ ¼
        report_parts.append(self._generate_simplified_summary_table(evaluations, scoring_dimensions))
        
        # 3. æ¨èæ€»ç»“
        report_parts.append(self._generate_recommendation_summary(evaluations))
        
        return "\n\n".join(report_parts)
    
    def _generate_simplified_summary_table(self, evaluations: List[CandidateEvaluation], scoring_dimensions: ScoringDimensions) -> str:
        """ç”Ÿæˆç®€åŒ–çš„å€™é€‰äººè¯„ä¼°æ±‡æ€»è¡¨æ ¼"""
        if not evaluations:
            return ""
        
        # è¡¨å¤´
        header = "| å€™é€‰äºº | ç»¼åˆå¾—åˆ† | æŠ€æœ¯èƒ½åŠ› | é¡¹ç›®ç»éªŒ | å›¢é˜Ÿç®¡ç† | ä¸»è¦ä¼˜åŠ¿ | ä¸»è¦ä¸è¶³ |"
        separator = "|--------|----------|----------|----------|----------|----------|----------|"
        
        rows = []
        
        for eval in evaluations:
            # å€™é€‰äººåç§°ï¼ˆåŠ ç²—æ’åå‰ä¸‰ï¼‰
            if eval.ranking == 1:
                candidate_name = f"**{eval.candidate_name}**"
            else:
                candidate_name = f"**{eval.candidate_name}**"
            
            # ç»¼åˆå¾—åˆ†ï¼ˆåŠ ç²—ï¼‰
            overall_score = f"**{eval.overall_score:.1f}/10**"
            
            # æå–å„ç»´åº¦å¾—åˆ†
            tech_score = self._extract_dimension_score(eval, ["æŠ€èƒ½åŒ¹é…", "æŠ€æœ¯èƒ½åŠ›", "æŠ€æœ¯æŠ€èƒ½"])
            project_score = self._extract_dimension_score(eval, ["ç»éªŒè¯„ä¼°", "é¡¹ç›®ç»éªŒ", "å·¥ä½œç»éªŒ"])
            management_score = self._extract_dimension_score(eval, ["è½¯æŠ€èƒ½", "å›¢é˜Ÿç®¡ç†", "ç®¡ç†èƒ½åŠ›"])
            
            # ä¸»è¦ä¼˜åŠ¿ï¼ˆå–å‰2ä¸ªï¼‰
            strengths = "ï¼Œ".join(eval.strengths[:2]) if eval.strengths else "åŸºç¡€æŠ€èƒ½"
            
            # ä¸»è¦ä¸è¶³ï¼ˆå–å‰1ä¸ªï¼‰
            weaknesses = eval.weaknesses[0] if eval.weaknesses else "å¾…äº†è§£"
            
            row = f"| {candidate_name} | {overall_score} | {tech_score}/10 | {project_score}/10 | {management_score}/10 | {strengths} | {weaknesses} |"
            rows.append(row)
        
        table = "\n".join([header, separator] + rows)
        
        # æ·»åŠ æ¨èç»“æœ
        recommendation_results = "\n\n**æ¨èç»“æœï¼š**\n"
        for i, eval in enumerate(evaluations[:3]):  # åªæ˜¾ç¤ºå‰3å
            if i == 0:
                emoji = "ğŸ¥‡"
                desc = "å¼ºçƒˆæ¨èï¼Œæœ€ä½³å€™é€‰äºº"
            elif i == 1:
                emoji = "ğŸ¥ˆ"
                desc = "æ¨èï¼Œæ¬¡é€‰å€™é€‰äºº"
            else:
                emoji = "ğŸ¥‰"
                desc = "å¯è€ƒè™‘ï¼Œå¤‡é€‰å€™é€‰äºº"
            
            if eval.overall_score >= SCORE_THRESHOLDS["RECOMMENDED"]:
                recommendation_results += f"{emoji} **{eval.candidate_name}** - {desc}\n"
            elif eval.overall_score >= SCORE_THRESHOLDS["CONSIDER"]:
                recommendation_results += f"âš ï¸ **{eval.candidate_name}** - è°¨æ…è€ƒè™‘ï¼Œéœ€è¿›ä¸€æ­¥è¯„ä¼°\n"
            else:
                recommendation_results += f"âŒ **{eval.candidate_name}** - ä¸æ¨èï¼Œä¸ç¬¦åˆè¦æ±‚\n"
        
        # å¤„ç†å‰©ä½™å€™é€‰äºº
        for eval in evaluations[3:]:
            if eval.overall_score >= SCORE_THRESHOLDS["RECOMMENDED"]:
                recommendation_results += f"âœ… **{eval.candidate_name}** - æ¨è\n"
            elif eval.overall_score >= SCORE_THRESHOLDS["CONSIDER"]:
                recommendation_results += f"âš ï¸ **{eval.candidate_name}** - è°¨æ…è€ƒè™‘\n"
            else:
                recommendation_results += f"âŒ **{eval.candidate_name}** - ä¸æ¨è\n"
        
        return f"## å€™é€‰äººè¯„ä¼°æ±‡æ€»\n\n{table}{recommendation_results}"
    
    def _extract_dimension_score(self, evaluation: CandidateEvaluation, dimension_names: List[str]) -> str:
        """æå–ç»´åº¦å¾—åˆ†"""
        for dimension_name in dimension_names:
            for score in evaluation.dimension_scores:
                if dimension_name in score.dimension_name:
                    return f"{score.score:.1f}"
        return "N/A"
    
    def _generate_header(self, job_requirement: JobRequirement, candidate_count: int) -> str:
        """ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        must_have_formatted = self._format_requirement_list(job_requirement.must_have)
        nice_to_have_formatted = self._format_requirement_list(job_requirement.nice_to_have)
        deal_breaker_formatted = self._format_requirement_list(job_requirement.deal_breaker)
        
        return REPORT_HEADER_TEMPLATE.format(
            position=job_requirement.position,
            candidate_count=candidate_count,
            current_time=current_time,
            must_have_formatted=must_have_formatted,
            nice_to_have_formatted=nice_to_have_formatted,
            deal_breaker_formatted=deal_breaker_formatted
        )
    
    def _format_requirement_list(self, requirements: List[str]) -> str:
        """æ ¼å¼åŒ–éœ€æ±‚åˆ—è¡¨"""
        if not requirements:
            return "- æ— "
        return "\n".join(f"- {req}" for req in requirements)
    
    def _generate_basic_info_table(self, evaluations: List[CandidateEvaluation]) -> str:
        """ç”ŸæˆåŸºæœ¬ä¿¡æ¯è¡¨æ ¼"""
        if not evaluations:
            return ""
        
        # è¡¨å¤´
        candidates = [f"**{eval.candidate_name}**" for eval in evaluations]
        header = "| **Basic Info** | " + " | ".join(candidates) + " |"
        separator = "|" + "|".join(["---"] * (len(candidates) + 1)) + "|"
        
        rows = []
        
        # æ’åè¡Œ
        rankings = [str(eval.ranking) for eval in evaluations]
        rows.append("| **Ranking** | " + " | ".join(rankings) + " |")
        
        # æ€»åˆ†è¡Œ
        scores = [f"{eval.overall_score:.1f}/10" for eval in evaluations]
        rows.append("| **Overall Score** | " + " | ".join(scores) + " |")
        
        # æ¨èçŠ¶æ€è¡Œ
        recommendations = []
        for eval in evaluations:
            if eval.overall_score >= SCORE_THRESHOLDS["RECOMMENDED"]:
                recommendations.append(RECOMMENDATION_STATUS["RECOMMENDED"])
            elif eval.overall_score >= SCORE_THRESHOLDS["CONSIDER"]:
                recommendations.append(RECOMMENDATION_STATUS["CONSIDER"])
            else:
                recommendations.append(RECOMMENDATION_STATUS["NOT_RECOMMENDED"])
        
        rows.append("| **Recommendation** | " + " | ".join(recommendations) + " |")
        
        # å°è¯•ä»ç»´åº¦è¯„åˆ†ä¸­æå–åŸºæœ¬ä¿¡æ¯
        basic_info_fields = ["å§“å", "ç»éªŒå¹´é™", "å½“å‰èŒä½", "æ•™è‚²èƒŒæ™¯", "æ‰€åœ¨åœ°"]
        
        for field in basic_info_fields:
            field_data = self._extract_field_data(evaluations, field)
            if field_data:
                rows.append(f"| **{field}** | " + " | ".join(field_data) + " |")
        
        table = "\n".join([header, separator] + rows)
        return BASIC_INFO_TABLE_TEMPLATE.format(table_content=table)
    
    def _generate_dimension_table(self, 
                                evaluations: List[CandidateEvaluation], 
                                dimension) -> str:
        """ç”Ÿæˆç»´åº¦è¯„åˆ†è¡¨æ ¼"""
        if not evaluations:
            return ""
        
        # æŸ¥æ‰¾è¯¥ç»´åº¦çš„è¯„åˆ†
        dimension_scores = []
        for eval in evaluations:
            dim_score = None
            for score in eval.dimension_scores:
                if score.dimension_name == dimension.name:
                    dim_score = score
                    break
            dimension_scores.append(dim_score)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ç»´åº¦çš„è¯„åˆ†ï¼Œè·³è¿‡
        if not any(score for score in dimension_scores):
            return ""
        
        # è¡¨å¤´
        candidates = [f"**{eval.candidate_name}**" for eval in evaluations]
        header = f"| **{dimension.name}** | " + " | ".join(candidates) + " |"
        separator = "|" + "|".join(["---"] * (len(candidates) + 1)) + " |"
        
        rows = []
        
        # ç»´åº¦æ€»åˆ†è¡Œ
        scores = []
        for score in dimension_scores:
            if score:
                score_str = f"{score.score:.1f} {score.status.value}"
                scores.append(score_str)
            else:
                scores.append("N/A")
        
        rows.append(f"| **{dimension.name} Score** | " + " | ".join(scores) + " |")
        
        # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆè¡Œ
        for field in dimension.fields:
            field_data = []
            for score in dimension_scores:
                if score and field in score.details:
                    detail = score.details[field]
                    # ç®€åŒ–æ˜¾ç¤ºï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯
                    field_data.append(self._format_field_detail(detail))
                else:
                    field_data.append("N/A")
            
            rows.append(f"| **{field}** | " + " | ".join(field_data) + " |")
        
        table = "\n".join([header, separator] + rows)
        weight_percentage = f"{dimension.weight:.0%}"
        return DIMENSION_TABLE_TEMPLATE.format(
            dimension_name=dimension.name,
            weight_percentage=weight_percentage,
            table_content=table
        )
    
    def _generate_overall_ranking_table(self, evaluations: List[CandidateEvaluation]) -> str:
        """ç”Ÿæˆæ€»ä½“æ’åè¡¨æ ¼"""
        if not evaluations:
            return ""
        
        # è¡¨å¤´
        header = "| **Rank** | **Candidate** | **Score** | **Status** | **Strengths** | **Weaknesses** |"
        separator = "|" + "|".join(["---"] * 6) + "|"
        
        rows = []
        for eval in evaluations:
            # æ¨èçŠ¶æ€
            if eval.overall_score >= SCORE_THRESHOLDS["RECOMMENDED"]:
                status = RECOMMENDATION_STATUS["RECOMMENDED"]
            elif eval.overall_score >= SCORE_THRESHOLDS["CONSIDER"]:
                status = RECOMMENDATION_STATUS["CONSIDER"]
            else:
                status = RECOMMENDATION_STATUS["NOT_RECOMMENDED"]
            
            # ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            strengths = ", ".join(eval.strengths[:2]) if eval.strengths else "æ— "
            weaknesses = ", ".join(eval.weaknesses[:2]) if eval.weaknesses else "æ— "
            
            row = f"| {eval.ranking} | {eval.candidate_name} | {eval.overall_score:.1f}/10 | {status} | {strengths} | {weaknesses} |"
            rows.append(row)
        
        table = "\n".join([header, separator] + rows)
        return OVERALL_RANKING_TEMPLATE.format(table_content=table)
    
    def _generate_recommendation_summary(self, evaluations: List[CandidateEvaluation]) -> str:
        """ç”Ÿæˆæ¨èæ€»ç»“"""
        if not evaluations:
            return ""
        
        # ç»Ÿè®¡æ¨èæƒ…å†µ
        recommended = [e for e in evaluations if e.overall_score >= SCORE_THRESHOLDS["RECOMMENDED"]]
        consider = [e for e in evaluations if SCORE_THRESHOLDS["CONSIDER"] <= e.overall_score < SCORE_THRESHOLDS["RECOMMENDED"]]
        not_recommended = [e for e in evaluations if e.overall_score < SCORE_THRESHOLDS["CONSIDER"]]
        
        summary_parts = []
        summary_parts.append(f"**è¯„ä¼°æ€»ç»“ï¼š**")
        summary_parts.append(f"- æ€»å€™é€‰äººï¼š{len(evaluations)} äºº")
        summary_parts.append(f"- æ¨èï¼š{len(recommended)} äºº")
        summary_parts.append(f"- è€ƒè™‘ï¼š{len(consider)} äºº")
        summary_parts.append(f"- ä¸æ¨èï¼š{len(not_recommended)} äºº")
        summary_parts.append("")
        
        if recommended:
            summary_parts.append("**æ¨èå€™é€‰äººï¼š**")
            for eval in recommended:
                summary_parts.append(f"- {eval.candidate_name} (å¾—åˆ†: {eval.overall_score:.1f})")
            summary_parts.append("")
        
        if consider:
            summary_parts.append("**å¯è€ƒè™‘å€™é€‰äººï¼š**")
            for eval in consider:
                summary_parts.append(f"- {eval.candidate_name} (å¾—åˆ†: {eval.overall_score:.1f})")
            summary_parts.append("")
        
        # æ·»åŠ æ€»ä½“å»ºè®®
        if recommended:
            summary_parts.append("**å»ºè®®ï¼š**")
            summary_parts.append("1. ä¼˜å…ˆé¢è¯•æ¨èå€™é€‰äºº")
            if consider:
                summary_parts.append("2. å¯è€ƒè™‘é¢è¯•éƒ¨åˆ†è€ƒè™‘å€™é€‰äºº")
            summary_parts.append("3. æ ¹æ®é¢è¯•ç»“æœæœ€ç»ˆç¡®å®šäººé€‰")
        else:
            summary_parts.append("**å»ºè®®ï¼š**")
            summary_parts.append("1. å½“å‰å€™é€‰äººæ•´ä½“åŒ¹é…åº¦ä¸é«˜")
            summary_parts.append("2. å»ºè®®æ‰©å¤§æ‹›è˜èŒƒå›´æˆ–è°ƒæ•´æ‹›è˜è¦æ±‚")
            summary_parts.append("3. å¯è€ƒè™‘é¢è¯•éƒ¨åˆ†è€ƒè™‘å€™é€‰äºº")
        
        summary_content = "\n".join(summary_parts)
        return RECOMMENDATION_SUMMARY_TEMPLATE.format(summary_content=summary_content)
    
    def _extract_field_data(self, evaluations: List[CandidateEvaluation], field_name: str) -> List[str]:
        """ä»ç»´åº¦è¯„åˆ†ä¸­æå–å­—æ®µæ•°æ®"""
        field_data = []
        for eval in evaluations:
            found = False
            for score in eval.dimension_scores:
                if field_name in score.details:
                    field_data.append(self._format_field_detail(score.details[field_name]))
                    found = True
                    break
            if not found:
                field_data.append("N/A")
        return field_data
    
    def _format_field_detail(self, detail: str) -> str:
        """æ ¼å¼åŒ–å­—æ®µè¯¦æƒ…"""
        if not detail:
            return "N/A"
        
        # é™åˆ¶é•¿åº¦ï¼Œé¿å…è¡¨æ ¼è¿‡å®½
        if len(detail) > 50:
            return detail[:47] + "..."
        return detail
    
    def run_standalone(self, 
                      evaluations: List[CandidateEvaluation],
                      job_requirement: JobRequirement,
                      scoring_dimensions: ScoringDimensions) -> str:
        """ç‹¬ç«‹è¿è¡Œæ¨¡å¼"""
        result = self.process(evaluations, job_requirement, scoring_dimensions)
        
        if result["status"] == "success":
            print(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {result['candidate_count']} ä¸ªå€™é€‰äºº")
            return result["report"]
        else:
            print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {result['error']}")
            return ""
    
    def save_report(self, report: str, filename: str = None) -> str:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"candidate_evaluation_report_{timestamp}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return filename
        except Exception as e:
            print(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return ""

def main():
    """æµ‹è¯•å‡½æ•°"""
    from src.models import CandidateEvaluation, JobRequirement, ScoringDimensions, ScoringDimension, DimensionScore, EvaluationStatus
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    evaluations = [
        CandidateEvaluation(
            candidate_id="1",
            candidate_name="å¼ ä¸‰",
            dimension_scores=[
                DimensionScore(
                    dimension_name="æŠ€èƒ½åŒ¹é…",
                    score=8.5,
                    status=EvaluationStatus.PASS,
                    details={"Python": "ç†Ÿç»ƒ", "Django": "ç†Ÿç»ƒ"},
                    comments="æŠ€èƒ½åŒ¹é…åº¦é«˜"
                )
            ],
            overall_score=8.5,
            recommendation="æ¨èé¢è¯•",
            strengths=["æŠ€æœ¯èƒ½åŠ›å¼º", "ç»éªŒä¸°å¯Œ"],
            weaknesses=["æ²Ÿé€šèƒ½åŠ›å¾…æå‡"]
        ),
        CandidateEvaluation(
            candidate_id="2", 
            candidate_name="æå››",
            dimension_scores=[
                DimensionScore(
                    dimension_name="æŠ€èƒ½åŒ¹é…",
                    score=6.0,
                    status=EvaluationStatus.WARNING,
                    details={"Python": "ä¸€èˆ¬", "Django": "ä¸€èˆ¬"},
                    comments="æŠ€èƒ½åŒ¹é…åº¦ä¸€èˆ¬"
                )
            ],
            overall_score=6.0,
            recommendation="å¯è€ƒè™‘",
            strengths=["å­¦ä¹ èƒ½åŠ›å¼º"],
            weaknesses=["ç»éªŒä¸è¶³"]
        )
    ]
    
    job_requirement = JobRequirement(
        position="Pythonå¼€å‘å·¥ç¨‹å¸ˆ",
        must_have=["Python", "Django"],
        nice_to_have=["Redis", "Docker"],
        deal_breaker=["æ— ç¼–ç¨‹ç»éªŒ"]
    )
    
    scoring_dimensions = ScoringDimensions(
        dimensions=[
            ScoringDimension(
                name="æŠ€èƒ½åŒ¹é…",
                weight=0.6,
                fields=["Python", "Django"],
                description="æŠ€æœ¯æŠ€èƒ½åŒ¹é…åº¦"
            )
        ]
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    node = ReportGenerationNode()
    report = node.run_standalone(evaluations, job_requirement, scoring_dimensions)
    
    if report:
        print("=== ç”Ÿæˆçš„æŠ¥å‘Š ===")
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        node.save_report(report)

if __name__ == "__main__":
    main()